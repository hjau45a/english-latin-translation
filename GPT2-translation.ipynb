{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a50d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pip install torch==2.2.0 \\\n",
    "            transformers==4.41.0 \\\n",
    "            datasets==2.19.0 \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4356482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f6420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'c:/Users/raven/Nextcloud/Documents/Development/translation'\n",
    "model_name = f\"{BASE_DIR}/gpt2-medium\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de3a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": ['<|stop|>']})\n",
    "stop_token_id = tokenizer.convert_tokens_to_ids('<|stop|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd390f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', 50256, '<|stop|>', 50257, 50257)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id, tokenizer.additional_special_tokens[0], tokenizer.convert_tokens_to_ids(tokenizer.additional_special_tokens[0]), stop_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f4dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 has 24 transformer blocks; freezing first -1\n"
     ]
    }
   ],
   "source": [
    "NUM_BLOCKS = len(model.transformer.h)   # total blocks (12 for base GPT‑2)\n",
    "freeze_up_to = -1\n",
    "\n",
    "print(f\"GPT2 has {NUM_BLOCKS} transformer blocks; freezing first {freeze_up_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4188fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 292/292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def freeze_gpt2_layers(model, n_blocks):\n",
    "    \"\"\"\n",
    "    Freeze the first `n_blocks` transformer blocks + the embedding matrices.\n",
    "    \"\"\"\n",
    "    # Freeze token & position embeddings (optional but common)\n",
    "\n",
    "    if n_blocks < 0:\n",
    "        trainable = sum(p.requires_grad for p in model.parameters())\n",
    "        total = sum(1 for _ in model.parameters())\n",
    "        print(f\"Trainable params: {trainable}/{total}\")\n",
    "        return None\n",
    "    \n",
    "    for param in model.transformer.wte.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.transformer.wpe.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3b️⃣ Freeze the first `n_blocks` blocks\n",
    "    for block_idx in range(n_blocks):\n",
    "        block = model.transformer.h[block_idx]\n",
    "        for name, param in block.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            # Uncomment the next line for a quick sanity print:\n",
    "            # print(f\"Freezing {name} in block {block_idx}\")\n",
    "\n",
    "    # 3c️⃣ Verify which parameters are still trainable\n",
    "    trainable = sum(p.requires_grad for p in model.parameters())\n",
    "    total = sum(1 for _ in model.parameters())\n",
    "    print(f\"Trainable params: {trainable}/{total}\")\n",
    "\n",
    "# Apply the freezing\n",
    "freeze_gpt2_layers(model, freeze_up_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d8a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is hosted on the Hub; no local files are needed.\n",
    "# It contains two fields: \"latin\" (source) and \"english\" (target).\n",
    "raw = datasets.load_dataset(\"grosenthal/latin_english_translation\")\n",
    "\n",
    "# The hub version already provides a train/validation split.\n",
    "# If you want a test split as well, you can further split the validation set.\n",
    "raw = raw[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_raw = raw[\"train\"].shuffle()\n",
    "test_raw  = raw[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8576c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 94664,\n",
       " 'la': 'et corruent singuli super fratres suos quasi bella fugientes nemo vestrum inimicis audebit resistere',\n",
       " 'en': 'And they shall every one fall upon their brethren as fleeing from wars: none of you shall dare to resist your enemies.',\n",
       " 'file': 'final_alignments\\\\Vulgate_Bible.json'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ce4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = 'translate English to Latin'\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe2fb901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867c3ca48e6149cdb7e8916c5ae55212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc52d6e9ca9343288a348cd0f3d59a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9935 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_example(example):\n",
    "    # Example prompt – feel free to change the wording or language pair\n",
    "    prompt = f\"{INSTRUCTION}: {example['en']}\"\n",
    "    # The model will see: \"<prompt> <example>\"\n",
    "    response = f\"{example['la']}\"\n",
    "    # Tokenise the input prompt\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized_response = tokenizer(\n",
    "        response,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    tokenized_response_input_ids = tokenized_response['input_ids']  # this is a list\n",
    "\n",
    "    try:\n",
    "        first_eos_token = tokenized_response_input_ids.index(tokenizer.eos_token_id)\n",
    "        tokenized_response_input_ids[first_eos_token] = stop_token_id\n",
    "        tokenized_response_input_ids = [x if x != tokenizer.eos_token_id else -100 for x in tokenized_response_input_ids]\n",
    "    except ValueError:\n",
    "        # There is no first eos_token_id: we ran out of space with our max_length\n",
    "        # Don't do anything.\n",
    "        pass\n",
    "\n",
    "    example['input_ids'] = tokenized_prompt['input_ids']\n",
    "    example['labels'] = tokenized_response_input_ids\n",
    "    return example\n",
    "\n",
    "train_dataset = train_raw.select(range(10000)).map(build_example, remove_columns=[\"en\", \"la\"])\n",
    "test_dataset  = test_raw.map(build_example,  remove_columns=[\"en\", \"la\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8061737",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,                 # not masked LM – we keep causal LM behavior\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34966aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 1024)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))   # in case we added a pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c3a8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=f\"{BASE_DIR}/finetuned-gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,               # start small; increase if needed\n",
    "    per_device_train_batch_size=4,    # fits on a 12 GB GPU; adjust as memory allows\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,    # effective batch size = 32\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                        # mixed‑precision speeds up training on RTX/AMP GPUs\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f525f070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 14:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.252500</td>\n",
       "      <td>3.154493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.043500</td>\n",
       "      <td>3.138791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.864500</td>\n",
       "      <td>3.150563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=939, training_loss=3.081868936459477, metrics={'train_runtime': 858.5029, 'train_samples_per_second': 34.945, 'train_steps_per_second': 1.094, 'total_flos': 6965255208960000.0, 'train_loss': 3.081868936459477, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19e7e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you been a good boy? No, I have not been one. But I am glad that you are alive, because, if I had not been, I should not have been able to write this. It is true that I am rather alone in this, since you have been my only companion, and it is better than no one. I am not sure how it happened, but I do not believe that you are so wretched that you have been unable to bear a letter. I should like to send you a letter, but I am afraid that I shall have to wait until next summer. Tell me, my dear friend, what\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence: str, max_new_tokens: int = 128):\n",
    "    prompt = f\"{INSTRUCTION}: {sentence}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # Generate until EOS or max_new_tokens\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=stop_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,               # you can switch to greedy (do_sample=False)\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    # Decode everything after the prompt\n",
    "    output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    # Remove the prompt part\n",
    "    return output[len(prompt):].strip()\n",
    "\n",
    "# Example\n",
    "print(translate(\"How are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c8aa3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 18337,\n",
       " 'la': 'Verum ea re tot res sunt, uti bene deicias, et suave est',\n",
       " 'en': 'but there are so many ingredients in this concoction that it is an excellent purgative, and, besides, it is agreeable.',\n",
       " 'file': 'final_alignments\\\\Cato_Agriculture.json'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0da83e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the Lord shall take you by the hand, and you shall die. And you shall be dashed with a sword, and shall die. But I will protect you, and will spare you. And your enemies shall be destroyed, and you shall find no place to escape. And I will hear your prayers, and will deliver you out of my hand. And you shall be delivered into my hand. And you shall escape into your own country. And my God will save you out of my hand. And your enemies shall be destroyed, and you shall find no place to escape. And I will bring you out of the midst of your enemies\n"
     ]
    }
   ],
   "source": [
    "print(translate(train_raw[0]['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "498bcf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The story of the black cat goes so: One day, a man was sitting by a stream and when he noticed a cat'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer('The story of the black cat goes so:', return_tensors=\"pt\").to(\"cuda\")\n",
    "# Generate until EOS or max_new_tokens\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=16,\n",
    "    eos_token_id=stop_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,               # you can switch to greedy (do_sample=False)\n",
    "    temperature=0.7,\n",
    ")\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "575dec10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2061,  318,  366, 9246,    1,  287, 2679,   30,  843,  644,  318,  262,\n",
       "         3616,  286,  262, 2456, 3797,  290, 3290,  287, 9133,   30,  554, 5486]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
